{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MY470 Computer Programming\n",
    "\n",
    "### Week 7 Assignment, MT 2021\n",
    "\n",
    "#### \\*\\*\\* Due 12:00 noon on Monday, November 15 \\*\\*\\*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "### Writing your own binary classifier evaluation module\n",
    "\n",
    "Classification is the supervised-learning equivalent of clustering, which we briefly touched on in Assignment 4. A classifier is used to label an observation as belonging to one of a finite number of categories, or classes. We use labeled training data to build the classifier and then use the classifier to predict the categories that new observations belong to. \n",
    "\n",
    "In this assignment, we will build our own module for working with and evaluating binary classifiers and write unit tests for it. We will then use the module to evaluate k-nearest neighbor (KNN) classification on social data. We will use the scikit-learn package to run the KNN algorithm.  \n",
    "\n",
    "We will use data from the file \"house-votes-84.data\". The file contains information on the voting record from the 1984 United States Congress. Our goal will be to predict whether the voter is a Democrat based on how they voted on 16 separate occasions. The data are obtained from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php) and you can find more information about them in the file \"house-votes-84.names\" as well as [here](http://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records).\n",
    "\n",
    "#### Hints\n",
    "\n",
    "**This assignment aims to test your mastery of function specifications, exceptions, assertions, and unit testing. Make sure your code handles different possible inputs that are not explicitly excluded in the function specification and deals appropriately with likely errors.**   \n",
    "\n",
    "For this assignment, you will be working with a partner. One reasonable way to divide the labor is to have partner A complete problems 1–4 and partner B complete problem 5. Another possibility is to split problems 1–4 and then work together on the tests (problem 5), where each partner is responsible for writing the unit tests for the functions they wrote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Function to split data into training and testing sets\n",
    "\n",
    "Create a .py file called `class_eval.py` and in it write a function called `split_training_testing` that takes as arguments a list of data points and a number indicating the percent of data to be used for testing. The function should randomly assign data points to either the training or testing sets, where the size of the sets is determined by the percent passed as an argument. The function should return the training and testing sets, in that particular order.\n",
    "\n",
    "#### Hints\n",
    "\n",
    "Although the terms refer to \"set\", your function should return the data as type `list`. \n",
    "\n",
    "You can use this notebook to write and test your code but for your final submission, please make sure the code is in `class_eval.py` and not here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Function to estimate values from confusion matrix\n",
    "\n",
    "In the file `class_eval.py`, write a function called `confusion_matrix` that takes as arguments a list of predicted values, a list of actual values, and a value indicating which class is the positive class and returns the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN), in that specific order. Here is how a confusion matrix looks like, assuming that 1 is the positive class and 0 is the negative class:\n",
    "\n",
    "|             | Actual 1 | Actual 0   \n",
    "|:-----------:|:--------:|:----------------------\n",
    "| **Predicted 1** | True positive | False positive             \n",
    "| **Predicted 0** | False negative | True negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#predicted_values = [1, 1, 1, 0 ,0, 1, 0, 0]\n",
    "#actual_values = [1,0]\n",
    "\n",
    "#expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "#predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
    "#Positive = 1\n",
    "#Negative = 0\n",
    "#(Order: numTP, numFP, NumTN, numFN)):\n",
    "\n",
    "predicted_val = [1,0,1,0]\n",
    "actual_val = [1,0,0,1]\n",
    "\n",
    "def confusion_matrix(predicted_values, actual_values, positive_class):\n",
    "    \"\"\"Returns the number of TPs, FPs, TNs and FNs.\n",
    "    Takes a list of predicted values,predicted_values,list of actual values,actual_values, \n",
    "    and a value indicating which class is the positive class\"\"\" \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(predicted_values)):\n",
    "    # let's first see if it's a true (t) or false prediction (f)\n",
    "        if predicted_values[i] == actual_values[i]: # t?\n",
    "            if predicted_values[i] == 1: # tp\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if predicted_values[i] == 1: # fp\n",
    "                FP += 1\n",
    "            else: # fn\n",
    "                FN += 1\n",
    "\n",
    "    return TP, FP, TN , FN \n",
    "    \n",
    "    \n",
    "print(confusion_matrix(predicted_val,actual_val, 1))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Functions to estimate evaluation metrics\n",
    "\n",
    "In the file `class_eval.py`, write five functions that estimate the accuracy, sensitivity, specificity, positive predictive value, and negative predictive value. \n",
    "\n",
    "Accuracy is the proportion of predictions that are correct, namely: $accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$.\n",
    "\n",
    "Sensitivity (also known as recall) is the proportion of positives that are correctly identified as such: $sensitivity = \\frac{TP}{TP + FN}$.\n",
    "\n",
    "Specificity (also known as precision) is the proportion of negatives that are correctly identified as such: $specificity = \\frac{TN}{TN + FP}$.\n",
    "\n",
    "Positive predictive value is the probability that a data point identified as positive is truly such: $positive \\ predictive\\ value = \\frac{TP}{TP + FP}$.\n",
    "\n",
    "Negative predictive value is the probability that a data point identified as negative is truly such: $negative\\ predictive\\ value = \\frac{TN}{TN + FN}$.\n",
    "\n",
    "The functions should take the required arguments and return the estimate. Use the following names for your functions: `accuracy`, `sensitivity`, `specificity`, `pos_pred_val`, `neg_pred_val`.\n",
    "\n",
    "#### Hints\n",
    "\n",
    "The functions should return `float('nan')` if they encounter division by 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Function to print evaluation metrics\n",
    "\n",
    "In the file `class_eval.py`, write a function called `print_eval_metrics` that takes as arguments a list of predicted values, a list of actual values, and a value indicating which class is the positive class and prints the accuracy, the sensitivity, the specificity, the positive predictive value, and the negative predictive value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_metrics(predicted_values, actual_values, positive_class):\n",
    "    \"\"\"Prints the accuracy, the sensitivity, the specificity, \n",
    "    the positive predictive value, and the negative predictive value.\n",
    "    Takes predicted_values, actual_values, positive_class as arguments.\"\"\"\n",
    "    return print(accuracy), print(sensitivity), print(specifity), print(pos_pred_val), print(neg_pred_val)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Unit tests for the classification evaluation module\n",
    "\n",
    "Create a .py file called `tests.py` and use the module `unittest` to write tests for the appropriate functions above. Follow this week's materials from lecture and class to identify informative testing values. You can read more about the `unittest` module [here](https://docs.python.org/3/library/unittest.html).  \n",
    "\n",
    "#### Hints\n",
    "\n",
    "Feel free to look at the testing modules of recognized Python packages for inspiration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluating your module using KNN on real data\n",
    "\n",
    "The code below uses the functions you have written and scikit-learn to run the KNN algorithm on a dataset and evaluate its performance. \n",
    "\n",
    "We first define a function called `get_data` that opens the file \"house-votes-84.data\" and returns all the data in a list, where each item is a list starting with the political affiliation of the congress member and then including the voting decisions of that congress member. The voting decisions are saved as a list of `0`'s, `0.5`'s, and `1`'s, where `0` stands for `'y'` in the data, `1` stands for `'n'`, and `0.5` stands for `'?'`. \n",
    "\n",
    "We get the data and split them into a 80% training and 20% testing set. We then fit the KNN classifier on the training set and predict the labels for the testing set. We use the predicted values and the actual values from the testing set to evaluate the algorithm's performance, where we assume that the positive value is \"democrat\". You can read more about how to use scikit-learn's KNN [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). \n",
    "\n",
    "**In order to run the code, you first need to install scikit-learn.** There are two ways to do this:\n",
    "\n",
    "1. Open a Terminal window and type: `conda install scikit-learn`\n",
    "2. Alternatively, open Anaconda Navigator, go to Environments, select \"All\" from the drop-down menu and type \"scikit-learn\" in the field \"Search Packages\". Then select the package and click on \"Apply\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8850574712643678\n",
      "Sensitivity: 0.864406779661017\n",
      "Specificity: 0.9285714285714286\n",
      "Positive predictive value: 0.9622641509433962\n",
      "Negative predictive value: 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "import class_eval\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"Opens file house-votes-84.data and returns a list with \n",
    "    labels (political affiliations) and a list with \n",
    "    feature vectors (voting decisions). Voting decisions \n",
    "    are represented with 1 for yes, 0 for no, and 0.5 for neither.\n",
    "    \"\"\"\n",
    "    relabel = {'y': 1, 'n': 0, '?': 0.5}\n",
    "    data = []\n",
    "    for line in open('house-votes-84.data', 'r'):\n",
    "        strlst = line.strip().split(',')\n",
    "        toappend = [strlst[0]] + [relabel[i] for i in strlst[1:]]\n",
    "        data.append(toappend)\n",
    "    return data \n",
    "\n",
    "# Get the data\n",
    "data = get_data()\n",
    "\n",
    "# Split it into training and testing sets and separate labels from feature vectors\n",
    "train, test = class_eval.split_training_testing(data, p_test=20)\n",
    "train_labels = [i[0] for i in train]\n",
    "train_features = [i[1:] for i in train]\n",
    "test_actual_labels = [i[0] for i in test]\n",
    "test_features = [i[1:] for i in test]\n",
    "\n",
    "# Make an instance of the KNN classifier and fit a model to the training data\n",
    "neigh = KNeighborsClassifier(n_neighbors=11)\n",
    "neigh.fit(train_features, train_labels) \n",
    "\n",
    "# Predict the labels for the test data and evaluate the performance\n",
    "test_pred_labels = neigh.predict(test_features)\n",
    "# The predict() method returns an object of type numpy.ndarray, so\n",
    "# we will transform it to list to fit the function specification\n",
    "class_eval.print_eval_metrics(list(test_pred_labels), test_actual_labels, 'democrat')\n",
    "\n",
    "\n",
    "# This routine is meant for testing purposes only.\n",
    "# In an actual analysis, we will look more systematically for a k \n",
    "# that maximizes the model's accuracy. We will then use multiple rounds \n",
    "# of random partitioning and average the model's performance over all rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "| Problem | Mark     | Comment   \n",
    "|:-------:|:--------:|:----------------------\n",
    "| 1       |   /2    |              \n",
    "| 2       |   /2    | \n",
    "| 3       |   /2    | \n",
    "| 4       |   /1    | \n",
    "| 5       |   /5    | \n",
    "| Legibility |   /2    |\n",
    "| Modularity |   /4    |\n",
    "| Efficiency |   /2    |\n",
    "|**Total**|**/20**  | "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
